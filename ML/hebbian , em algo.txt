#--------------------Or gate--------------------------------

import numpy as np

def hebbian_learning(inputs, targets, learning_rate=0.1, epochs=1):
    num_inputs = inputs.shape[1]
    weights = np.zeros(num_inputs)

    for _ in range(epochs):
        for i in range(len(inputs)):
            weights += learning_rate * inputs[i] * targets[i]

    return weights

# OR GATE
inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
targets = np.array([0, 1, 1, 1])
targetsforand = np.array([0, 0, 0, 1])
output = hebbian_learning(inputs, targets, learning_rate=0.1, epochs=5)

print("Weights:", output)
print("OR GATE")
for i in range(len(inputs)):
    activation = np.dot(inputs[i], output)
    prediction = 1 if activation > 0 else 0
    print(f"Input: {inputs[i]}, Target: {targets[i]}, Prediction: {prediction}")


#-----------------------------and gate----------------------------------------

import numpy as np

def hebbian_learning(inputs, outputs, learning_rate=0.1, epochs=20):
    num_inputs = inputs.shape[1]
    weights = np.random.rand(num_inputs) 

    for epoch in range(epochs):
        for input, output in zip(inputs, outputs):
            neuron_output = 1 if np.dot(weights, input) > 0 else 0
            weights += learning_rate * input * (output - neuron_output)

    return weights

inputs = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])
outputs = np.array([0, 0, 0, 1])
weights = hebbian_learning(inputs, outputs)
print("Learned weights for AND gate:", weights)
print("\nPredictions:")
for input_data in inputs:
    activation = np.dot(weights, input_data)
    prediction = 1 if activation > 0 else 0
    print(f"Input: {input_data}, Prediction: {prediction}")


#------------------------Em Algorithm---------------------------------

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
n_samples = 300
n_features = 2
n_components = 2
random_state = 42

X, _ = make_blobs(n_samples=n_samples, centers=n_components, cluster_std=1.0, random_state=random_state)

def initialize_parameters(X, n_components):
    n_samples, n_features = X.shape
    np.random.seed(random_state)
    
    means = X[np.random.choice(n_samples, n_components, replace=False)]
    covariances = [np.cov(X.T)] * n_components  # Use the covariance of X as the starting covariance
    weights = np.ones(n_components) / n_components  # Equal weights initially
    
    return means, covariances, weights

# E-step: Calculate responsibilities
def e_step(X, means, covariances, weights):
    n_samples, n_components = X.shape[0], len(means)
    responsibilities = np.zeros((n_samples, n_components))
    
    for k in range(n_components):
        # Multivariate Gaussian distribution PDF
        diff = X - means[k]
        inv_cov = np.linalg.inv(covariances[k])
        det_cov = np.linalg.det(covariances[k])
        
        exp_term = np.exp(-0.5 * np.sum(diff @ inv_cov * diff, axis=1))
        norm_factor = 1 / np.sqrt((2 * np.pi) ** n_features * det_cov)
        responsibilities[:, k] = weights[k] * norm_factor * exp_term
    
    # Normalize responsibilities
    responsibilities /= responsibilities.sum(axis=1, keepdims=True)
    return responsibilities

# M-step: Update parameters
def m_step(X, responsibilities):
    n_samples, n_features = X.shape
    n_components = responsibilities.shape[1]
    
    # Update means, covariances, and weights
    means = np.dot(responsibilities.T, X) / responsibilities.sum(axis=0)[:, None]
    
    covariances = []
    for k in range(n_components):
        diff = X - means[k]
        weighted_diff = responsibilities[:, k][:, None] * diff
        covariances.append(np.dot(weighted_diff.T, diff) / responsibilities[:, k].sum())
    
    weights = responsibilities.sum(axis=0) / n_samples
    
    return means, covariances, weights

# Convergence check: Stop if the log-likelihood change is small
def log_likelihood(X, means, covariances, weights):
    n_samples, n_components = X.shape[0], len(means)
    log_likelihood = 0
    
    for k in range(n_components):
        diff = X - means[k]
        inv_cov = np.linalg.inv(covariances[k])
        det_cov = np.linalg.det(covariances[k])
        
        exp_term = np.exp(-0.5 * np.sum(diff @ inv_cov * diff, axis=1))
        norm_factor = 1 / np.sqrt((2 * np.pi) ** n_features * det_cov)
        
        log_likelihood += np.log(weights[k] * norm_factor * exp_term)
    
    return np.sum(log_likelihood)

def fit_gmm(X, n_components, max_iter=100, tol=1e-6):
    means, covariances, weights = initialize_parameters(X, n_components)
    prev_log_likelihood = None
    for i in range(max_iter):
        responsibilities = e_step(X, means, covariances, weights)
        means, covariances, weights = m_step(X, responsibilities)
        log_likelihood_value = log_likelihood(X, means, covariances, weights)
        if prev_log_likelihood is not None and abs(log_likelihood_value - prev_log_likelihood) < tol:
            print(f"Converged at iteration {i}")
            break
        prev_log_likelihood = log_likelihood_value
    
    return means, covariances, weights

means, covariances, weights = fit_gmm(X, n_components)

print("Estimated Means:\n", means)
print("Estimated Covariances:\n", covariances)
print("Estimated Weights:\n", weights)


